{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Neural network using MNIST data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = keras.datasets.mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will load the dataset into train and test part automatically\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the dataset\n",
    "x_train = (x_train/255).astype('float32')\n",
    "x_test = (x_test/255).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if we check the labels we will see it is not one hot encoded yet.\n",
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using this library we will convert the labels into one hot encodings.\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "y_train_hot = (lb.fit_transform(y_train)).astype('int32')\n",
    "y_test_hot = (lb.fit_transform(y_test)).astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 1, 0, 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now if we check the labels we will see that it is one hot encoded.\n",
    "y_train_hot[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spliting into train and validation sets\n",
    "x_train, x_val, y_train_hot, y_val_hot = train_test_split(x_train, y_train_hot, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*mnist* data set object has methods to call next batches. For our better understanding, we will not use the premade method, instead, we will make a function to call the next batch. The following funtion does the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as in batch calling we shuffle the data every time, that's why we did horizontal stacking so that the correct labels are not\n",
    "# messed up.\n",
    "def next_batch(x,y, batch_size):\n",
    "    x = x.reshape(-1,28*28)    \n",
    "    data = np.hstack((x,y))\n",
    "    np.random.shuffle(data)\n",
    "    x = data[:, :-10]\n",
    "    y = data[:,-10:]\n",
    "    x = x.reshape(-1,28,28,1)\n",
    "    x = (x[:batch_size,:]).astype(\"float32\")\n",
    "    y = (y[:batch_size,:]).astype(\"int32\")\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_batches = x_train.shape[0]/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshaping the data to feed into neural networks.\n",
    "x_test = x_test.reshape(-1,28,28,1)\n",
    "x_val = x_val.reshape(-1, 28,28,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we defined the computational graph below. Its a very simple neural network with two hidden layers. The advantage of using mid level API is that it gives the flexibility to modify layers and at the same time, avoids the complication of defining each layers (weights, biases etc.) manually. The layers are defined using one line here. So the computational graph has one input layer which is a flatten layer. We need to flatten the data before feeding it to a dense layer. Hence, the flatten layer. Then we have two dense layers each followed by a relu activation. and last the output layer has a SOFTMAX activation with number of classes as the output size. You ask why SOFTMAX? we want the probability of each data to belonging to one of the 10 classes (ten numbers from 0 to 9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"define the computational graph here\"\n",
    "\n",
    "tf.reset_default_graph()\n",
    "x = tf.placeholder(tf.float32, shape=(None, 28, 28, 1))\n",
    "y = tf.placeholder(tf.int32)\n",
    "kernel_initializer = tf.initializers.truncated_normal(stddev=0.1)\n",
    "\n",
    "flat = tf.layers.flatten(x)\n",
    "l1 = tf.layers.dense(flat,128, kernel_initializer=kernel_initializer)\n",
    "relu1 = tf.nn.relu(l1)\n",
    "l2 = tf.layers.dense(relu1,32, kernel_initializer=kernel_initializer)\n",
    "relu2 = tf.nn.relu(l2)\n",
    "\n",
    "logits = tf.layers.dense(relu2, 10, kernel_initializer=kernel_initializer)\n",
    "prob = tf.nn.softmax(logits)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=prob,labels=y))\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch loss: 672.3380879163742\n",
      "Accuracy after epoch 1: 0.9196969866752625\n",
      "epoch loss: 617.7031271457672\n",
      "Accuracy after epoch 2: 0.9382323026657104\n",
      "Test Accuracy  0.937\n"
     ]
    }
   ],
   "source": [
    "# we will just run a few epochs to demonstrate the process.\n",
    "num_ep = 2\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initializers.global_variables())\n",
    "    \n",
    "    for ep in range(num_ep):\n",
    "        ep_loss = 0\n",
    "        for steps in range(int(n_batches)):\n",
    "            x_batch, y_batch_hot = next_batch(x_train,y_train_hot, batch_size)\n",
    "            batch_loss, _ = sess.run([loss, training_op], feed_dict={x:x_batch,y:y_batch_hot})                                     \n",
    "            ep_loss = ep_loss+batch_loss\n",
    "            \n",
    "        correct = tf.equal(tf.argmax(prob, 1), tf.argmax(y,1))\n",
    "        accu = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "        accuracy = accu.eval({x:x_val, y:y_val_hot})\n",
    "        print(\"epoch loss: {}\".format(ep_loss))\n",
    "        print(\"Accuracy after epoch {}: {}\".format((ep+1),accuracy))\n",
    "    \n",
    "    # After the completed epochs we test the model    \n",
    "    test_correct = tf.equal(tf.argmax(prob, 1), tf.argmax(y,1))\n",
    "    test_accu = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    test_accuracy = test_accu.eval({x:x_test, y:y_test_hot})\n",
    "    print(\"Test Accuracy \", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
